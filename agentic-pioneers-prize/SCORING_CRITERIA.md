# Scoring Criteria - Agentic AI Pioneers Prize

## Assessment Overview

| Aspect | Details |
|--------|---------|
| **Scoring Range** | 1-10 per question |
| **Number of Assessors** | 3 |
| **Scored Questions** | Q10-Q15 (6 questions) |
| **Maximum Possible Score** | 180 (6 questions × 10 points × 3 assessors) |
| **Target Score Band** | 9-10 for each question |

---

## Q10: High Level Technical Approach (400 words + REQUIRED appendix)

**Prompt**: How does the system work and why is it credible?

### Must Explain
- The final system architecture, agent roles, key components and data flows
- What is novel compared with the state of the art and what was hard in practice
- The key performance metrics, datasets and test conditions used to validate the system
- The tools, compute and environments used during development and validation

### Appendix Requirement
- **Required**: Yes
- **Format**: PDF, max 10MB, 2 A4 pages
- **Content**: Diagrams, tables or other visuals that help explain technical approach and system performance

### Scoring Bands

#### 9-10 (Excellent)
- Technical approach is clear, coherent, and well justified
- Architecture, components and data flows are well explained and easy to understand
- Approach is clearly and convincingly aligned with the selected challenge statement and its objectives
- Where a multi-agent solution is claimed, agent roles and interactions are clearly described and evidenced
- Novelty is credible and well demonstrated
- Performance metrics are appropriate, validated and supported with convincing evidence
- Appendix significantly enhances clarity

#### 7-8 (Good)
- Technical approach is clearly described but may lack depth in some areas
- Architecture and data flows are understandable though less detailed
- Alignment to the selected challenge statement is generally sound but not strongly evidenced
- Multi-agent roles and interactions are described but not fully substantiated
- Novelty is present but less well substantiated
- Performance metrics are provided with reasonable but not comprehensive evidence

#### 5-6 (Adequate)
- Approach is described but lacks clarity, structure or detail
- Alignment with the challenge statement is weak, generic or implied
- Claims of multi-agent behaviour are vague or lightly evidenced
- Novelty claims are vague or weakly evidenced
- Performance evidence is limited or inconsistently presented

#### 3-4 (Weak)
- Technical description is unclear or incomplete
- Alignment with the selected challenge statement is poor or inconsistent
- Minimal or no meaningful novelty is demonstrated
- Little or no credible evidence that the technical approach can achieve stated objectives
- Performance evidence is weak, inappropriate or absent

#### 1-2 (Very Weak)
- Technical approach is fundamentally unclear or incoherent
- No credible evidence that the solution can meet the challenge objectives
- Multi-agent aspects are absent or not substantiated
- No credible performance evidence
- Appendix missing or unusable

---

## Q11: User and Workflow Fit (400 words + optional appendix)

**Prompt**: Who will use it, where does it fit in the workflow, and what does it improve?

### Must Explain
- The real user context, roles and tasks you are designing for, and any human-in-the-loop steps
- Potential implications for time, quality, cost, safety or reliability in that workflow
- Interoperability with existing tools, systems, data formats and governance constraints

### Appendix Requirement
- **Required**: No (optional)
- **Format**: PDF, max 10MB, 2 A4 pages
- **Content**: Visuals that clarify the user journey, workflow, or how solution fits into existing processes

### Scoring Bands

#### 9-10 (Excellent)
- User context and workflow are described with strong domain understanding
- Clear insight into real tasks, constraints, dependencies and typical organisational practices
- Proposed workflow integration is realistic, coherent and clearly linked to the selected challenge
- Human-in-the-loop roles are identified and well justified where relevant
- Interoperability with existing tools, systems and governance is addressed credibly
- Any assumptions about adoption or operation are explicit and plausible
- Appendix enhances clarity, if submitted

#### 7-8 (Good)
- User context and workflow are well described, with good domain understanding
- Workflow integration is plausible but some steps or constraints are not fully explored
- Human-in-the-loop roles are mentioned but not fully developed
- Interoperability is recognised with some relevant detail
- Assumptions about adoption are reasonable but not deeply examined

#### 5-6 (Adequate)
- User context is described but in a general or high-level way
- Workflow integration is partially explained or relies on broad assumptions
- Limited discussion of realistic constraints, dependencies or operational challenges
- Human-in-the-loop roles are only lightly touched on
- Interoperability is mentioned but lacks useful detail

#### 3-4 (Weak)
- Limited or shallow understanding of users, roles or workflows
- Proposed integration into real-world processes is unclear or unrealistic
- Human-in-the-loop needs are ignored or poorly justified
- Interoperability constraints are not recognised or described inaccurately

#### 1-2 (Very Weak)
- No meaningful understanding of users or workflows is demonstrated
- No credible explanation of how the solution would embed into real processes
- Appendix adds no value or is irrelevant, if submitted

---

## Q12: MVP and Integration Readiness (400 words + REQUIRED appendix)

**Prompt**: Describe what aspects of your solution are ready for demonstration and how they can be deployed.

### Must Explain
- The MVP scope delivered, what can be demonstrated, and how this aligns with the challenge statement
- Packaging, APIs, interfaces and deployment model validated during the phase
- Verification and validation results, acceptance thresholds met and known limitations
- What remains to reach production in a typical UK setting, and the plan to close gaps

### Appendix Requirement
- **Required**: Yes
- **Format**: PDF, max 10MB, 2 A4 pages
- **Content**: Diagrams, summaries or tables that help show what MVP does and how it can be deployed

### Scoring Bands

#### 9-10 (Excellent)
- Baseline capability and progression to current MVP are clearly and convincingly described
- MVP is well-defined, coherent, and strongly aligned with the challenge statement
- What can be demonstrated is clearly described and directly evidences progress
- Evidence of testing, demonstration and deployment is strong, relevant and robust
- Packaging, interfaces and APIs are clearly described and technically credible
- Verification and validation activities are substantial, appropriate and well evidenced
- Remaining gaps to production are clearly identified with realistic plans to close them
- Environmental and societal considerations are recognised and addressed
- Appendix is clear, well structured and materially enhances understanding

#### 7-8 (Good)
- Baseline and progression to MVP are described with generally good clarity
- MVP is clear and broadly aligned with the challenge statement
- Demonstrable capability shows plausible progress
- Testing, demonstration and deployment considerations are present but not fully developed
- Verification and validation activities are reasonable but incomplete
- Remaining gaps are identified with broadly realistic plans

#### 5-6 (Adequate)
- Baseline and progression are mentioned but in a general or superficial way
- MVP's alignment to the challenge statement is only loosely made or implied
- Demonstrable capability is unclear or weakly evidenced
- Evidence of deployment or integration is limited
- Verification and validation activities are modest or poorly connected to claims
- Remaining gaps are noted but plans are vague or unconvincing

#### 3-4 (Weak)
- MVP is unclear, inconsistently described or not credible
- Alignment with the challenge statement is weak or not clearly established
- Minimal evidence of testing, demonstration or deployment
- Verification and validation are weak, absent or inappropriate
- Remaining gaps are largely unrecognised or unrealistic

#### 1-2 (Very Weak)
- No meaningful MVP is described, or description is incoherent
- No credible link between what can be demonstrated and the challenge statement
- No credible evidence of testing, demonstration or deployment
- Required appendix is missing or unusable

---

## Q13: Risks, Assurance and Explainability (400 words + optional appendix)

**Prompt**: What could go wrong and how will you control it responsibly?

### Must Explain
- The principal technical, data, safety, security and delivery risks encountered, and mitigations
- Assurance activities completed (tests, red-teaming, evaluation protocols, audit trails)
- How the system explains its outputs or actions to users, and evidence this is usable
- Compliance status against relevant regulations or standards, and remaining actions

### Appendix Requirement
- **Required**: No (optional)
- **Format**: PDF, max 10MB, 2 A4 pages
- **Content**: Materials that summarise key risks, assurance activities or explainability approach

### Scoring Bands

#### 9-10 (Excellent)
- Key risks are clearly identified, relevant and thoughtfully analysed
- Mitigations are well justified, proportionate and clearly linked to identified risks
- Assurance activities (testing, evaluation, audit trails, red-teaming) are substantial and credible
- Data privacy, security, access control and logging are addressed clearly with realistic safeguards
- Potential misuse, unintended behaviours or harmful outputs are recognised with credible mitigation
- Explainability approach is suitable for intended users and supported by evidence
- Compliance with relevant regulations or standards is clearly described

#### 7-8 (Good)
- Main risks are identified with generally reasonable mitigations
- Assurance activities are present and broadly appropriate
- Data privacy and security are recognised with some relevant safeguards
- Misuse or harmful output risks are mentioned with some mitigation
- Explainability approach is appropriate but only lightly evidenced
- Regulatory or standards alignment is outlined but may lack detail

#### 5-6 (Adequate)
- Risks are acknowledged but analysis is limited or generic
- Mitigations are high-level or lack clear linkage to specific risks
- Assurance activities are modest or described at high level
- Data privacy and security are mentioned briefly or in generic terms
- Misuse, harmful behaviours, bias or fairness are only lightly referenced
- Explainability is described without convincing detail on suitability
- Regulatory or standards considerations are mentioned but unclear

#### 3-4 (Weak)
- Risks are weakly understood, incomplete or poorly prioritised
- Mitigations are vague, unrealistic or largely absent
- Little meaningful assurance activity is described
- Data privacy and security are largely overlooked
- Misuse, harmful output or fairness risks are not recognised
- Explainability approach is inappropriate or confusing
- Regulatory or standards aspects are not addressed

#### 1-2 (Very Weak)
- Major risks are unaddressed or not identified
- No credible mitigations or assurance activities
- No meaningful consideration of data privacy, security or misuse
- No explainability approach is described
- No recognition of relevant regulations or standards

---

## Q14: Potential Commercial Impact and UK Benefit (400 words, no appendix)

**Prompt**: What impact might this project have outside the project team?

### Must Explain
- Target customers, value proposition and evidence from this phase that supports ROI
- Route to adoption: partnerships, pricing, delivery and support model validated or de-risked
- Expected UK benefits such as productivity, cost reduction, quality, exports or resilience
- Competitive positioning and how the work strengthens UK capabilities and IP

### Scoring Bands

#### 9-10 (Excellent)
- Commercial proposition is clear, strong and well supported
- Evidence of demand or ROI is compelling
- Route to adoption is credible and informed by real partnerships or testing
- UK benefits are clearly articulated, specific and meaningful

#### 7-8 (Good)
- Commercial case is strong but some areas may lack detail
- Evidence of demand reasonable though not comprehensive
- UK benefits described but less fully developed

#### 5-6 (Adequate)
- Commercial case present but generic or thinly evidenced
- Adoption route unclear or high-level
- UK benefits limited or vague

#### 3-4 (Weak)
- Weak or unrealistic commercial case
- Little supporting evidence
- UK benefits unclear or unconvincing

#### 1-2 (Very Weak)
- No credible commercial potential
- No meaningful benefit to the UK

---

## Q15: Future Potential and Scalability (400 words, no appendix)

**Prompt**: How far do you think you can develop your solution?

### Must Explain
- The technical roadmap informed by the solution development phase
- Generalisability to adjacent use cases or sectors, and limits to transfer
- Scaling considerations for data, safety, compute, deployment and operations in UK settings
- Partnership, IP and commercial milestones you will pursue post-competition

### Scoring Bands

#### 9-10 (Excellent)
- Roadmap is clear, evidence-informed and realistic
- Strong potential for wider application or generalisation
- Scaling considerations are well understood and addressed
- Next steps, partnerships or milestones are credible and aligned to opportunity

#### 7-8 (Good)
- Roadmap reasonable with some evidence
- Some discussion of generalisation or scale though less detailed
- Next steps clear but modest

#### 5-6 (Adequate)
- Roadmap present but speculative or high-level
- Limited articulation of scalability or constraints
- Next steps unclear or generic

#### 3-4 (Weak)
- Roadmap lacks credibility or detail
- Scalability unexplored or unrealistic
- No meaningful next steps

#### 1-2 (Very Weak)
- No viable future development described
- No consideration of scale

---

## Scoring Strategy

To achieve 9-10 scores across all questions, each answer must:

1. **Be Specific**: Use concrete examples, metrics, and evidence - not vague generalities
2. **Align with Challenge**: Every claim must tie back to the Detailed Design for X challenge objectives
3. **Evidence-Based**: Include quantitative performance data, test results, user feedback
4. **Multi-Agent Focus**: Clearly articulate agent roles, interactions, and orchestration
5. **UK Context**: Emphasise UK benefits, compliance, and deployment considerations
6. **Professional Visuals**: Use appendices effectively with clear, professional diagrams

---
*Reference: agentic_aI_assessor_guideance_for_applicants.pdf*
